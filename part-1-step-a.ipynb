{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "Non-projective sentences:\n",
      "Total number of non-projective sentences: 0\n",
      "\n",
      "Exercise 2: Reordered sentences to make the tree projective:\n",
      "\n",
      "Exercise 3: Analysis of Non-Projective Sentences:\n",
      "\n",
      "## Most frequent syntactic construction type: Wh-movement/Fronting\n",
      "Examples:\n",
      "\n",
      "### Other construction types:\n",
      "1. Extraposition\n",
      "Examples:\n",
      "\n",
      "2. Parentheticals/Insertions\n",
      "Examples:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "#url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/dev/de_hdt-ud-dev.conllu'\n",
    "\n",
    "\n",
    "# MHD Bashir Kabbani 3017375\n",
    "# Vialli Tem Ndumbe 2911472 \n",
    "\n",
    "# Step 1: Download the treebank file\n",
    "url = 'https://github.com/UniversalDependencies/UD_German-HDT.git'\n",
    "response = requests.get(url)\n",
    "file_content = response.text\n",
    "\n",
    "# Step 2: Parse the file content\n",
    "def parse_conllu(file_content):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    sentence_meta = {}\n",
    "    for line in file_content.split('\\n'):\n",
    "        if line.startswith('#'):\n",
    "            if line.startswith('# sent_id'):\n",
    "                sentence_meta['id'] = line.split('=')[-1].strip()\n",
    "            elif line.startswith('# text'):\n",
    "                sentence_meta['text'] = line.split('=')[-1].strip()\n",
    "        elif line.strip() == '':\n",
    "            if sentence:\n",
    "                sentences.append((sentence_meta, sentence))\n",
    "                sentence = []\n",
    "                sentence_meta = {}\n",
    "        else:\n",
    "            sentence.append(line.split('\\t'))\n",
    "    print(len(sentences))\n",
    "    return sentences\n",
    "\n",
    "# Step 3: Identify non-projective trees\n",
    "def is_non_projective(sentence):\n",
    "    arcs = []\n",
    "    for token in sentence:\n",
    "        if len(token) < 8:\n",
    "            continue  # Skip tokens with fewer than 8 elements\n",
    "        try:\n",
    "            head = int(token[6])\n",
    "            dependent = int(token[0])\n",
    "        except ValueError:\n",
    "            continue  # Skip tokens where head or id is not an integer\n",
    "        if head == 0:  # Skip root\n",
    "            continue\n",
    "        if head > dependent:\n",
    "            head, dependent = dependent, head\n",
    "        arcs.append((head, dependent))\n",
    "    \n",
    "    for i in range(len(arcs)):\n",
    "        for j in range(i + 1, len(arcs)):\n",
    "            (h1, d1) = arcs[i]\n",
    "            (h2, d2) = arcs[j]\n",
    "            if (h1 < h2 < d1 < d2) or (h2 < h1 < d2 < d1):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Step 4: Extract non-projective sentences and their identifiers\n",
    "def extract_non_projective_sentences(file_content):\n",
    "    sentences = parse_conllu(file_content)\n",
    "    non_projective_sentences = []\n",
    "    for sentence_meta, sentence in sentences:\n",
    "        if is_non_projective(sentence):\n",
    "            non_projective_sentences.append((sentence_meta['id'], sentence_meta['text'], sentence))\n",
    "    return non_projective_sentences\n",
    "\n",
    "# Step 5: Main execution\n",
    "non_projective_sentences = extract_non_projective_sentences(file_content)\n",
    "print(\"Non-projective sentences:\")\n",
    "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
    "    print(f\"ID: {sentence_id}\")\n",
    "    print(f\"Text: {sentence_text}\")\n",
    "    print()\n",
    "\n",
    "# Step 5: Count non-projective sentences\n",
    "def count_non_projective_sentences(non_projective_sentences):\n",
    "    return len(non_projective_sentences)\n",
    "\n",
    "# Print the count of non-projective sentences\n",
    "total_non_projective = count_non_projective_sentences(non_projective_sentences)\n",
    "print(f\"Total number of non-projective sentences: {total_non_projective}\")\n",
    "\n",
    "# Exercise 2: Reordered list of words to make the tree projective\n",
    "def create_dependency_graph(sentence):\n",
    "    graph = defaultdict(list)\n",
    "    words = {}\n",
    "    for token in sentence:\n",
    "        if len(token) < 8:\n",
    "            continue\n",
    "        try:\n",
    "            head = int(token[6])\n",
    "            dependent = int(token[0])\n",
    "            word = token[1]\n",
    "        except ValueError:\n",
    "            continue  # Skip tokens where head or id is not an integer\n",
    "        graph[head].append(dependent)\n",
    "        words[dependent] = word\n",
    "    return graph, words\n",
    "\n",
    "def inorder_traversal(node, graph, words, result):\n",
    "    if node not in graph:\n",
    "        return\n",
    "    children = sorted(graph[node])\n",
    "    for child in children:\n",
    "        inorder_traversal(child, graph, words, result)\n",
    "    if node in words:\n",
    "        result.append(words[node])\n",
    "\n",
    "def make_projective(sentence):\n",
    "    graph, words = create_dependency_graph(sentence)\n",
    "    result = []\n",
    "    inorder_traversal(0, graph, words, result)\n",
    "    return result\n",
    "\n",
    "print(\"\\nExercise 2: Reordered sentences to make the tree projective:\")\n",
    "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
    "    projective_order = make_projective(sentence)\n",
    "    print(f\"ID: {sentence_id}\")\n",
    "    print(f\"Original Text: {sentence_text}\")\n",
    "    print(\"Projective Order:\", \" \".join(projective_order))\n",
    "    print()\n",
    "\n",
    "# Exercise 3: Analysis of syntactic construction types\n",
    "def analyze_construction_types(non_projective_sentences, num_sentences=50):\n",
    "    construction_types = {\n",
    "        'wh-movement/fronting': [],\n",
    "        'extraposition': [],\n",
    "        'parentheticals/insertions': []\n",
    "    }\n",
    "\n",
    "    for sentence_id, sentence_text, sentence in non_projective_sentences[:num_sentences]:\n",
    "        for token in sentence:\n",
    "            if len(token) >= 8:\n",
    "                construction = token[7]\n",
    "                if construction.startswith('wh'):\n",
    "                    construction_types['wh-movement/fronting'].append((sentence_id, sentence_text))\n",
    "                elif construction.startswith('extr'):\n",
    "                    construction_types['extraposition'].append((sentence_id, sentence_text))\n",
    "                elif construction.startswith('parataxis'):\n",
    "                    construction_types['parentheticals/insertions'].append((sentence_id, sentence_text))\n",
    "\n",
    "    return construction_types\n",
    "\n",
    "print(\"\\nExercise 3: Analysis of Non-Projective Sentences:\")\n",
    "construction_types = analyze_construction_types(non_projective_sentences)\n",
    "\n",
    "print(\"\\n## Most frequent syntactic construction type: Wh-movement/Fronting\")\n",
    "print(\"Examples:\")\n",
    "for sent_id, sent_text in construction_types['wh-movement/fronting']:\n",
    "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n",
    "\n",
    "print(\"\\n### Other construction types:\")\n",
    "print(\"1. Extraposition\")\n",
    "print(\"Examples:\")\n",
    "for sent_id, sent_text in construction_types['extraposition']:\n",
    "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n",
    "\n",
    "print(\"\\n2. Parentheticals/Insertions\")\n",
    "print(\"Examples:\")\n",
    "for sent_id, sent_text in construction_types['parentheticals/insertions']:\n",
    "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = []\n",
    "for f in os.listdir('.'):\n",
    "    if f.endswith('conllu'):\n",
    "        files_list.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de_dht-ud-train_conllu.conllu',\n",
       " 'de_hdt-ud-dev.conllu',\n",
       " 'de_hdt-ud-dev_filtered.conllu',\n",
       " 'de_hdt-ud-test.conllu',\n",
       " 'de_hdt-ud-test_filtered.conllu',\n",
       " 'de_hdt-ud-train-a-1.conllu',\n",
       " 'de_hdt-ud-train-a-1_filtered.conllu',\n",
       " 'de_hdt-ud-train-a-2.conllu',\n",
       " 'de_hdt-ud-train-a-2_filtered.conllu',\n",
       " 'de_hdt-ud-train-b-1.conllu',\n",
       " 'de_hdt-ud-train-b-1_filtered.conllu',\n",
       " 'de_hdt-ud-train-b-2.conllu',\n",
       " 'de_hdt-ud-train-b-2_filtered.conllu']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n",
      "de_dht-ud-train_conllu.conllu\n",
      "0\n",
      "===========================\n",
      "18434\n",
      "de_hdt-ud-dev.conllu\n",
      "1240\n",
      "===========================\n",
      "17194\n",
      "de_hdt-ud-dev_filtered.conllu\n",
      "0\n",
      "===========================\n",
      "18459\n",
      "de_hdt-ud-test.conllu\n",
      "1208\n",
      "===========================\n",
      "17251\n",
      "de_hdt-ud-test_filtered.conllu\n",
      "0\n",
      "===========================\n",
      "38102\n",
      "de_hdt-ud-train-a-1.conllu\n",
      "2322\n",
      "===========================\n",
      "35780\n",
      "de_hdt-ud-train-a-1_filtered.conllu\n",
      "0\n",
      "===========================\n",
      "37515\n",
      "de_hdt-ud-train-a-2.conllu\n",
      "2652\n",
      "===========================\n",
      "34863\n",
      "de_hdt-ud-train-a-2_filtered.conllu\n",
      "0\n",
      "===========================\n",
      "38411\n",
      "de_hdt-ud-train-b-1.conllu\n",
      "2687\n",
      "===========================\n",
      "35724\n",
      "de_hdt-ud-train-b-1_filtered.conllu\n",
      "0\n",
      "===========================\n",
      "39007\n",
      "de_hdt-ud-train-b-2.conllu\n",
      "2717\n",
      "===========================\n",
      "36290\n",
      "de_hdt-ud-train-b-2_filtered.conllu\n",
      "0\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "for file_url in files_list:\n",
    "    with open(file_url, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        non_projective_sentences = extract_non_projective_sentences(''.join(lines))\n",
    "        \n",
    "        print(file_url)\n",
    "        print(len(non_projective_sentences))\n",
    "        print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sents(lines):\n",
    "    c = 0\n",
    "    for line in lines:\n",
    "        if 'sent_id' in line:\n",
    "            c = c + 1\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sentences(input_file, output_file, ids_to_remove):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.read().strip().split('\\n\\n')\n",
    "    \n",
    "    remaining_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        lines = sentence.split('\\n')\n",
    "        sent_id = None\n",
    "        \n",
    "        # Find the sent_id in the current sentence\n",
    "        for line in lines:\n",
    "            if line.startswith('# sent_id ='):\n",
    "                sent_id = line.split('=')[1].strip()\n",
    "                break\n",
    "        \n",
    "        # If the sent_id is not in the list to remove, keep the sentence\n",
    "        if sent_id not in ids_to_remove:\n",
    "            remaining_sentences.append(sentence)\n",
    "    \n",
    "    # Write the remaining sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n\\n'.join(remaining_sentences) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [\n",
    " 'de_hdt-ud-dev.conllu',\n",
    " 'de_hdt-ud-test.conllu',\n",
    " 'de_hdt-ud-train-a-1.conllu',\n",
    " 'de_hdt-ud-train-a-2.conllu',\n",
    " 'de_hdt-ud-train-b-1.conllu',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_hdt-ud-dev.conllu\n",
      "18434\n",
      "de_hdt-ud-test.conllu\n",
      "18459\n",
      "de_hdt-ud-train-a-1.conllu\n",
      "38102\n",
      "de_hdt-ud-train-a-2.conllu\n",
      "37515\n",
      "de_hdt-ud-train-b-1.conllu\n",
      "38411\n"
     ]
    }
   ],
   "source": [
    "for file in m:\n",
    "    print(file)\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    non_projective_sentences = extract_non_projective_sentences(''.join(lines))\n",
    "\n",
    "    input_file = file\n",
    "    output_file = file.replace('.conllu', '_filtered.conllu')\n",
    "    ids_to_remove = [tup[0] for tup in non_projective_sentences]\n",
    "    remove_sentences(input_file, output_file, ids_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
